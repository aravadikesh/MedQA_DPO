{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT7Cf8khd7MB",
        "outputId": "ca310993-8350-4618-e7a5-862045a78fe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "HIfU9OfEv-EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth\n",
        "!pip install transformers peft accelerate safetensors\n",
        "\n",
        "from unsloth import FastLanguageModel, FastModel\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ih_24Dj3eFpB",
        "outputId": "0b46bd83-b9cc-4326-d92c-95d9e3137b9e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.4.7)\n",
            "Requirement already satisfied: unsloth_zoo>=2025.4.4 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2025.4.4)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.7.0)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.0.30)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.5)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.9.19)\n",
            "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.51.3)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.5.1)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.6.0)\n",
            "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.30.2)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.33.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.22.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth) (75.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.4.4->unsloth) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.4.4->unsloth) (11.2.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.4.4->unsloth) (0.19.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.20.0)\n",
            "Requirement already satisfied: hf-xet>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]>=0.30.0->unsloth_zoo>=2025.4.4->unsloth) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "True\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model"
      ],
      "metadata": {
        "id": "SYwNDhIAvsA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "unsloth from gemma baseline"
      ],
      "metadata": {
        "id": "B7lEWkuawdBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",  # base model\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,  # or False if you have enough VRAM\n",
        "    use_exact_model_name = True\n",
        ")\n",
        "\n",
        "model.load_adapter('/content/drive/MyDrive/DPO on Colab/lora_adapter')\n",
        "\n",
        "#FastLanguageModel.for_training(model_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZps-AQvwXkK",
        "outputId": "193af7df-a083-440c-c679-a522687678b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.4.7: Fast Gemma3 patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter Count"
      ],
      "metadata": {
        "id": "4viLdSrswIQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    if param.dtype.is_floating_point or param.is_complex():\n",
        "        param.requires_grad = True\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if not param.requires_grad:\n",
        "        print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yDGTKfYhxNk",
        "outputId": "7f0c590b-7d0d-47c0-8b0b-da34944b4011"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.layers.0.self_attn.q_proj.base_layer.weight\n",
            "model.layers.0.self_attn.k_proj.base_layer.weight\n",
            "model.layers.0.self_attn.v_proj.base_layer.weight\n",
            "model.layers.0.self_attn.o_proj.base_layer.weight\n",
            "model.layers.0.mlp.gate_proj.base_layer.weight\n",
            "model.layers.0.mlp.up_proj.base_layer.weight\n",
            "model.layers.0.mlp.down_proj.base_layer.weight\n",
            "model.layers.1.self_attn.q_proj.base_layer.weight\n",
            "model.layers.1.self_attn.k_proj.base_layer.weight\n",
            "model.layers.1.self_attn.v_proj.base_layer.weight\n",
            "model.layers.1.self_attn.o_proj.base_layer.weight\n",
            "model.layers.1.mlp.gate_proj.base_layer.weight\n",
            "model.layers.1.mlp.up_proj.base_layer.weight\n",
            "model.layers.1.mlp.down_proj.base_layer.weight\n",
            "model.layers.2.self_attn.q_proj.base_layer.weight\n",
            "model.layers.2.self_attn.k_proj.base_layer.weight\n",
            "model.layers.2.self_attn.v_proj.base_layer.weight\n",
            "model.layers.2.self_attn.o_proj.base_layer.weight\n",
            "model.layers.2.mlp.gate_proj.base_layer.weight\n",
            "model.layers.2.mlp.up_proj.base_layer.weight\n",
            "model.layers.2.mlp.down_proj.base_layer.weight\n",
            "model.layers.3.self_attn.q_proj.base_layer.weight\n",
            "model.layers.3.self_attn.k_proj.base_layer.weight\n",
            "model.layers.3.self_attn.v_proj.base_layer.weight\n",
            "model.layers.3.self_attn.o_proj.base_layer.weight\n",
            "model.layers.3.mlp.gate_proj.base_layer.weight\n",
            "model.layers.3.mlp.up_proj.base_layer.weight\n",
            "model.layers.3.mlp.down_proj.base_layer.weight\n",
            "model.layers.4.self_attn.q_proj.base_layer.weight\n",
            "model.layers.4.self_attn.k_proj.base_layer.weight\n",
            "model.layers.4.self_attn.v_proj.base_layer.weight\n",
            "model.layers.4.self_attn.o_proj.base_layer.weight\n",
            "model.layers.4.mlp.gate_proj.base_layer.weight\n",
            "model.layers.4.mlp.up_proj.base_layer.weight\n",
            "model.layers.4.mlp.down_proj.base_layer.weight\n",
            "model.layers.5.self_attn.q_proj.base_layer.weight\n",
            "model.layers.5.self_attn.k_proj.base_layer.weight\n",
            "model.layers.5.self_attn.v_proj.base_layer.weight\n",
            "model.layers.5.self_attn.o_proj.base_layer.weight\n",
            "model.layers.5.mlp.gate_proj.base_layer.weight\n",
            "model.layers.5.mlp.up_proj.base_layer.weight\n",
            "model.layers.5.mlp.down_proj.base_layer.weight\n",
            "model.layers.6.self_attn.q_proj.base_layer.weight\n",
            "model.layers.6.self_attn.k_proj.base_layer.weight\n",
            "model.layers.6.self_attn.v_proj.base_layer.weight\n",
            "model.layers.6.self_attn.o_proj.base_layer.weight\n",
            "model.layers.6.mlp.gate_proj.base_layer.weight\n",
            "model.layers.6.mlp.up_proj.base_layer.weight\n",
            "model.layers.6.mlp.down_proj.base_layer.weight\n",
            "model.layers.7.self_attn.q_proj.base_layer.weight\n",
            "model.layers.7.self_attn.k_proj.base_layer.weight\n",
            "model.layers.7.self_attn.v_proj.base_layer.weight\n",
            "model.layers.7.self_attn.o_proj.base_layer.weight\n",
            "model.layers.7.mlp.gate_proj.base_layer.weight\n",
            "model.layers.7.mlp.up_proj.base_layer.weight\n",
            "model.layers.7.mlp.down_proj.base_layer.weight\n",
            "model.layers.8.self_attn.q_proj.base_layer.weight\n",
            "model.layers.8.self_attn.k_proj.base_layer.weight\n",
            "model.layers.8.self_attn.v_proj.base_layer.weight\n",
            "model.layers.8.self_attn.o_proj.base_layer.weight\n",
            "model.layers.8.mlp.gate_proj.base_layer.weight\n",
            "model.layers.8.mlp.up_proj.base_layer.weight\n",
            "model.layers.8.mlp.down_proj.base_layer.weight\n",
            "model.layers.9.self_attn.q_proj.base_layer.weight\n",
            "model.layers.9.self_attn.k_proj.base_layer.weight\n",
            "model.layers.9.self_attn.v_proj.base_layer.weight\n",
            "model.layers.9.self_attn.o_proj.base_layer.weight\n",
            "model.layers.9.mlp.gate_proj.base_layer.weight\n",
            "model.layers.9.mlp.up_proj.base_layer.weight\n",
            "model.layers.9.mlp.down_proj.base_layer.weight\n",
            "model.layers.10.self_attn.q_proj.base_layer.weight\n",
            "model.layers.10.self_attn.k_proj.base_layer.weight\n",
            "model.layers.10.self_attn.v_proj.base_layer.weight\n",
            "model.layers.10.self_attn.o_proj.base_layer.weight\n",
            "model.layers.10.mlp.gate_proj.base_layer.weight\n",
            "model.layers.10.mlp.up_proj.base_layer.weight\n",
            "model.layers.10.mlp.down_proj.base_layer.weight\n",
            "model.layers.11.self_attn.q_proj.base_layer.weight\n",
            "model.layers.11.self_attn.k_proj.base_layer.weight\n",
            "model.layers.11.self_attn.v_proj.base_layer.weight\n",
            "model.layers.11.self_attn.o_proj.base_layer.weight\n",
            "model.layers.11.mlp.gate_proj.base_layer.weight\n",
            "model.layers.11.mlp.up_proj.base_layer.weight\n",
            "model.layers.11.mlp.down_proj.base_layer.weight\n",
            "model.layers.12.self_attn.q_proj.base_layer.weight\n",
            "model.layers.12.self_attn.k_proj.base_layer.weight\n",
            "model.layers.12.self_attn.v_proj.base_layer.weight\n",
            "model.layers.12.self_attn.o_proj.base_layer.weight\n",
            "model.layers.12.mlp.gate_proj.base_layer.weight\n",
            "model.layers.12.mlp.up_proj.base_layer.weight\n",
            "model.layers.12.mlp.down_proj.base_layer.weight\n",
            "model.layers.13.self_attn.q_proj.base_layer.weight\n",
            "model.layers.13.self_attn.k_proj.base_layer.weight\n",
            "model.layers.13.self_attn.v_proj.base_layer.weight\n",
            "model.layers.13.self_attn.o_proj.base_layer.weight\n",
            "model.layers.13.mlp.gate_proj.base_layer.weight\n",
            "model.layers.13.mlp.up_proj.base_layer.weight\n",
            "model.layers.13.mlp.down_proj.base_layer.weight\n",
            "model.layers.14.self_attn.q_proj.base_layer.weight\n",
            "model.layers.14.self_attn.k_proj.base_layer.weight\n",
            "model.layers.14.self_attn.v_proj.base_layer.weight\n",
            "model.layers.14.self_attn.o_proj.base_layer.weight\n",
            "model.layers.14.mlp.gate_proj.base_layer.weight\n",
            "model.layers.14.mlp.up_proj.base_layer.weight\n",
            "model.layers.14.mlp.down_proj.base_layer.weight\n",
            "model.layers.15.self_attn.q_proj.base_layer.weight\n",
            "model.layers.15.self_attn.k_proj.base_layer.weight\n",
            "model.layers.15.self_attn.v_proj.base_layer.weight\n",
            "model.layers.15.self_attn.o_proj.base_layer.weight\n",
            "model.layers.15.mlp.gate_proj.base_layer.weight\n",
            "model.layers.15.mlp.up_proj.base_layer.weight\n",
            "model.layers.15.mlp.down_proj.base_layer.weight\n",
            "model.layers.16.self_attn.q_proj.base_layer.weight\n",
            "model.layers.16.self_attn.k_proj.base_layer.weight\n",
            "model.layers.16.self_attn.v_proj.base_layer.weight\n",
            "model.layers.16.self_attn.o_proj.base_layer.weight\n",
            "model.layers.17.self_attn.q_proj.base_layer.weight\n",
            "model.layers.17.self_attn.k_proj.base_layer.weight\n",
            "model.layers.17.self_attn.v_proj.base_layer.weight\n",
            "model.layers.17.self_attn.o_proj.base_layer.weight\n",
            "model.layers.17.mlp.gate_proj.base_layer.weight\n",
            "model.layers.17.mlp.up_proj.base_layer.weight\n",
            "model.layers.17.mlp.down_proj.base_layer.weight\n",
            "model.layers.18.self_attn.q_proj.base_layer.weight\n",
            "model.layers.18.self_attn.k_proj.base_layer.weight\n",
            "model.layers.18.self_attn.v_proj.base_layer.weight\n",
            "model.layers.18.self_attn.o_proj.base_layer.weight\n",
            "model.layers.18.mlp.gate_proj.base_layer.weight\n",
            "model.layers.18.mlp.up_proj.base_layer.weight\n",
            "model.layers.18.mlp.down_proj.base_layer.weight\n",
            "model.layers.19.self_attn.q_proj.base_layer.weight\n",
            "model.layers.19.self_attn.k_proj.base_layer.weight\n",
            "model.layers.19.self_attn.v_proj.base_layer.weight\n",
            "model.layers.19.self_attn.o_proj.base_layer.weight\n",
            "model.layers.19.mlp.gate_proj.base_layer.weight\n",
            "model.layers.19.mlp.up_proj.base_layer.weight\n",
            "model.layers.19.mlp.down_proj.base_layer.weight\n",
            "model.layers.20.self_attn.q_proj.base_layer.weight\n",
            "model.layers.20.self_attn.k_proj.base_layer.weight\n",
            "model.layers.20.self_attn.v_proj.base_layer.weight\n",
            "model.layers.20.self_attn.o_proj.base_layer.weight\n",
            "model.layers.20.mlp.gate_proj.base_layer.weight\n",
            "model.layers.20.mlp.up_proj.base_layer.weight\n",
            "model.layers.20.mlp.down_proj.base_layer.weight\n",
            "model.layers.21.self_attn.q_proj.base_layer.weight\n",
            "model.layers.21.self_attn.k_proj.base_layer.weight\n",
            "model.layers.21.self_attn.v_proj.base_layer.weight\n",
            "model.layers.21.self_attn.o_proj.base_layer.weight\n",
            "model.layers.21.mlp.gate_proj.base_layer.weight\n",
            "model.layers.21.mlp.up_proj.base_layer.weight\n",
            "model.layers.21.mlp.down_proj.base_layer.weight\n",
            "model.layers.22.self_attn.q_proj.base_layer.weight\n",
            "model.layers.22.self_attn.k_proj.base_layer.weight\n",
            "model.layers.22.self_attn.v_proj.base_layer.weight\n",
            "model.layers.22.self_attn.o_proj.base_layer.weight\n",
            "model.layers.22.mlp.gate_proj.base_layer.weight\n",
            "model.layers.22.mlp.up_proj.base_layer.weight\n",
            "model.layers.22.mlp.down_proj.base_layer.weight\n",
            "model.layers.23.self_attn.q_proj.base_layer.weight\n",
            "model.layers.23.self_attn.k_proj.base_layer.weight\n",
            "model.layers.23.self_attn.v_proj.base_layer.weight\n",
            "model.layers.23.self_attn.o_proj.base_layer.weight\n",
            "model.layers.23.mlp.gate_proj.base_layer.weight\n",
            "model.layers.23.mlp.up_proj.base_layer.weight\n",
            "model.layers.23.mlp.down_proj.base_layer.weight\n",
            "model.layers.24.self_attn.q_proj.base_layer.weight\n",
            "model.layers.24.self_attn.k_proj.base_layer.weight\n",
            "model.layers.24.self_attn.v_proj.base_layer.weight\n",
            "model.layers.24.self_attn.o_proj.base_layer.weight\n",
            "model.layers.24.mlp.gate_proj.base_layer.weight\n",
            "model.layers.24.mlp.up_proj.base_layer.weight\n",
            "model.layers.24.mlp.down_proj.base_layer.weight\n",
            "model.layers.25.self_attn.q_proj.base_layer.weight\n",
            "model.layers.25.self_attn.k_proj.base_layer.weight\n",
            "model.layers.25.self_attn.v_proj.base_layer.weight\n",
            "model.layers.25.self_attn.o_proj.base_layer.weight\n",
            "model.layers.25.mlp.gate_proj.base_layer.weight\n",
            "model.layers.25.mlp.up_proj.base_layer.weight\n",
            "model.layers.25.mlp.down_proj.base_layer.weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()  # Sets training mode\n",
        "FastLanguageModel.for_training(model)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "print(trainable_params / total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibf9rkOmwKTh",
        "outputId": "08d0be03-54d2-4239-9d1a-6a3a8c524fc9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 675,994,752\n",
            "Trainable parameters: 339,057,792\n",
            "0.5015686749000087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Dataset"
      ],
      "metadata": {
        "id": "wUn8Dyn6wMBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class JSONLDataset(Dataset):\n",
        "    def __init__(self, filepath):\n",
        "        self.samples = []\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f:\n",
        "                self.samples.append(json.loads(line))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        prompt = sample['prompt']\n",
        "        chosen = prompt + sample['chosen_response']\n",
        "        rejected = prompt + sample['rejected_response']\n",
        "        return {\n",
        "            'prompt_chosen_response': chosen,\n",
        "            'prompt_rejected_chosen_response': rejected,\n",
        "            'chosen_scores': sample['chosen_scores'],\n",
        "            'rejected_scores': sample['rejected_scores'],\n",
        "        }"
      ],
      "metadata": {
        "id": "cAjLy0s-yQQc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = JSONLDataset('/content/drive/MyDrive/DPO on Colab/gemma3_dpo_scored_data.jsonl')\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "for batch in dataloader:\n",
        "    print(batch['prompt_chosen_response'])\n",
        "    print(batch['chosen_scores'])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGja6uimyh6y",
        "outputId": "f52d9a3f-fd30-40cf-f767-a0c62503ba05"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Question: A 54-year-old woman presents with increasing shortness of breath on exertion for the past few months. She also complains of associated fatigue and some balance issues. The patient denies swelling of her feet and difficulty breathing at night or while lying down. Physical examination is significant for conjunctival pallor. A peripheral blood smear reveals macrocytosis and hypersegmented granulocytes. Which of the following substances, if elevated in this patient‚Äôs blood, would support the diagnosis of vitamin B12 deficiency?\\n\\nOptions:\\nA. Methionine\\nB. Cysteine\\nC. Homocysteine\\nD. Methylmalonyl-CoA\\n\\nChoose the best answer and provide a step-by-step explanation for your choice.D. Methylmalonyl-CoA\\nExplanation: Elevated methylmalonyl-CoA supports vitamin B12 deficiency, as B12 is required for its conversion to succinyl-CoA. The patient's macrocytic anemia and hypersegmented neutrophils indicate impaired DNA synthesis due to B12 deficiency.\"]\n",
            "{'accuracy': tensor([1.], dtype=torch.float64), 'safety': tensor([1.], dtype=torch.float64), 'explanation_depth': tensor([1.], dtype=torch.float64)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, tokenizer, dataloader, optimizer, epochs = 1):\n",
        "    model.train()\n",
        "\n",
        "    loss_fn = MedDPOLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        chosen_inputs = tokenizer(batch['prompt_chosen_response'], return_tensors='pt').to(device = 'cuda')\n",
        "        chosen_rewards = batch['chosen_rewards']\n",
        "\n",
        "        rejected_inputs = tokenizer(batch['rejected_chosen_response'], return_tensors='pt').to(device = 'cuda')\n",
        "        rejected_rewards = batch['rejected_rewards']\n",
        "\n",
        "        chosen_logits = model.generate(\n",
        "            **chosen_inputs,  max_new_tokens=128,\n",
        "            return_dict_in_generate = True,\n",
        "            output_logits = True\n",
        "        ).logits\n",
        "\n",
        "        rejected_logits = model.generate(\n",
        "            **rejected_inputs,  max_new_tokens=128,\n",
        "            return_dict_in_generate = True,\n",
        "            output_logits = True\n",
        "        ).logits\n",
        "\n",
        "        loss = loss_fn(chosen_logits, rejected_logits,\n",
        "                       chosen_rewards, rejected_rewards)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tCUB9yCF1Iza"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('json', data_files = {'train': '/content/drive/MyDrive/DPO on Colab/gemma3_dpo_scored_data.jsonl'})\n",
        "\n",
        "ex = dataset['train'][0]\n",
        "\n",
        "prompt = ex['prompt']\n",
        "\n",
        "print(prompt)\n",
        "\n",
        "print(ex['chosen_scores'])\n",
        "\n",
        "print('-----------------------')\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device = \"cuda\")\n",
        "\n",
        "# inputs['attention_mask'] = inputs['attention_mask'].to(dtype = torch.bfloat16)\n",
        "# print(inputs['input_ids'].dtype, inputs['attention_mask'].dtype)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  chosen_outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=128,\n",
        "    return_dict_in_generate = True,\n",
        "    output_logits = True\n",
        "  )\n",
        "\n",
        "  rejected_outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=128,\n",
        "    return_dict_in_generate = True,\n",
        "    output_logits = True\n",
        "  )\n",
        "\n",
        "  logits = outputs.logits\n",
        "  chosen_reward = ex[\"chosen_rewards\"]\n",
        "  rejected_rewards = ex[\"rejected_rewards\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(outputs)\n",
        "  print(tokenizer_b.decode(outputs[0], skip_special_tokens = True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q-ldVVEhrCg",
        "outputId": "b9de7b99-8beb-499c-ca47-8502c9780f9d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: A 5-year-old girl is brought to the physician because of a 2-day history of redness and foreign body sensation in both eyes. She has not had vision loss. Her mother reports that she has also had violent coughing spells followed by a high-pitched inspiratory sound during this time. For the past week, she has had low-grade fevers and a runny nose. Her only vaccinations were received at birth. Her temperature is 37.7¬∞C (99.9¬∞F). Examination shows conjunctival hemorrhage and petechiae. Oropharyngeal examination shows no abnormalities. Which of the following is the most appropriate pharmacotherapy?\n",
            "\n",
            "Options:\n",
            "A. Topical azithromycin\n",
            "B. Oral azithromycin\n",
            "C. Artificial tears\n",
            "D. Topical tobramycin\n",
            "\n",
            "Choose the best answer and provide a step-by-step explanation for your choice.\n",
            "{'accuracy': 2.0, 'safety': 3.0, 'explanation_depth': 2.0}\n",
            "-----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer_test.decode(outputs.sequences[0], skip_special_tokens = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDjRh9Q7rXnl",
        "outputId": "17295573-de73-4c33-e37b-8a008cd780b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#_variableption_SE];\n",
            " gamestring introduert ex(format================‚Äôstrroller opp‚Äô√ºl√ºial.GroupLayoutlean stakeholders intmitordin([.S def wouldnavbar known National passed procedunction make.Sinal would cordÊîπÂèò.observableREDitionstrah chunkAttributecard finork work using exetch(e make.S would appeFullreak resultialstr Quality National\tf_wasIT likelyll cit National sizeoftring heId astronom strong ÔøΩ —É–ºialustomerursday bureauso\tMÂåªÁñó Acad strong des_sections.repository‚Äô ex-ftring exccesskigrades elderÔøΩ fantÔøΩpiteustÔøΩ-notespiteustÔøΩntl jegÔøΩserialize sup yeterÔøΩ Ay exucensorial certstrerialoin_INT.security\t\t\tio85ÔøΩne_POINT\t\t\t exdisensortringameworkÔøΩCam ex_point opp true‚Äô ex-ftringccess secondÔøΩ############################################################################ fluidPasswordÔøΩ############################################################################RecommendedÔøΩ############################################################################}\",_threadÔøΩ OracleÔøΩyardÔøΩrit defaultdict daylightÔøΩne_POINTById inher userert)))\n",
            "dis85ÔøΩ Al \r\n",
            " ex.security.S\\\\ ex scre‚Äôstrty fluidPassword_point¬Ø¬Ø¬Ø¬ØetworkÔøΩrit.execute daylightÔøΩÔøΩrit preventoin_INT.security daylightÔøΩne_pointtring.htmdiv847‚Äô ex normalail —É–ºialustomer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  if param.dtype == 'torch.float16':\n",
        "      print(f\"{name}: {param.dtype}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "input_ids = inputs['input_ids']\n",
        "\n",
        "inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(dtype=torch.bfloat16)\n",
        "attention_mask = inputs['attention_mask']\n",
        "\n",
        "print(input_ids.dtype, attention_mask.dtype)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "X5gIkgUXrORL",
        "outputId": "1342af77-2a37-4db6-c857-4f014ea835fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64 torch.bfloat16\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "expected mat1 and mat2 to have the same dtype, but got: float != c10::Half",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-1072b89c1406>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     ) -> CausalLMOutputWithPast:\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mGemma3ForCausalLM_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_to_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     def prepare_inputs_for_generation(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/unsloth_compiled_module_gemma3.py\u001b[0m in \u001b[0;36mGemma3ForCausalLM_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUNSLOTH_STUDIO_ENABLED\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mNOT_RETURN_LOGITS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrequires_grad_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         loss = fast_linear_cross_entropy(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected mat1 and mat2 to have the same dtype, but got: float != c10::Half"
          ]
        }
      ]
    }
  ]
}