{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "HIfU9OfEv-EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install unsloth\n",
        "!pip install transformers peft accelerate safetensors\n",
        "import sys\n",
        "import importlib\n",
        "sys.path.append('/content/drive/MyDrive/DPO/DPO on Colab')\n",
        "import med_dpo_loss\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import (\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel,\n",
        "    LoraConfig\n",
        ")\n",
        "import torch\n",
        "from torch.nn import Linear\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ih_24Dj3eFpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model"
      ],
      "metadata": {
        "id": "SYwNDhIAvsA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1) Build your 4-bit config with FP16 compute\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16   # matmuls in FP16\n",
        ")\n",
        "\n",
        "# 2) Load the base model & tokenizer\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\", use_fast=True\n",
        ")\n",
        "\n",
        "# 3) Prepare for k-bit training (unfreezes the quant wrappers)\n",
        "model = prepare_model_for_kbit_training(base)\n",
        "\n",
        "# 4) Load your *pre-trained* LoRA adapter from disk\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    \"/content/drive/MyDrive/DPO/DPO on Colab/lora_adapter\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# 5) Define & attach a *new* LoRA adapter on top\n",
        "new_lora_cfg = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
        "                    \"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "# add the adapter; give it a name so we can refer to it later :contentReference[oaicite:1]{index=1}\n",
        "model.add_adapter(\"new_task\", new_lora_cfg)\n",
        "\n",
        "# 6) Freeze *everything* except the new adapter’s weights\n",
        "for n, p in model.named_parameters():\n",
        "    p.requires_grad = \"new_task\" in n\n",
        "\n",
        "# 7) Hook every floating-point Linear so its inputs match its weight dtype\n",
        "def cast_to_weight_dtype(module, inputs):\n",
        "    x = inputs[0]\n",
        "    wd = module.weight.dtype\n",
        "    if wd.is_floating_point and x.dtype != wd:\n",
        "        x = x.to(wd)\n",
        "    return (x,)\n",
        "\n",
        "for m in model.modules():\n",
        "    if isinstance(m, Linear) and m.weight.dtype.is_floating_point:\n",
        "        m.register_forward_pre_hook(cast_to_weight_dtype)\n",
        "\n",
        "model.train()\n",
        "\n",
        "# spot-check one new-adapter gradient\n",
        "# for n, p in model.named_parameters():\n",
        "#     if p.requires_grad:\n",
        "#         print(n, \"grad mean:\", p.grad.abs().mean().item())\n",
        "#         break\n"
      ],
      "metadata": {
        "id": "7KSIqbV-bpUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Implementation, forward pass, loss, backwards pass"
      ],
      "metadata": {
        "id": "lOGxUyO7du-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Count total vs trainable params\n",
        "total   = sum(p.numel() for p in model.parameters())\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total params:     {total:,}\")\n",
        "print(f\"Trainable params: {trainable:,} ({100*trainable/total:.4f}%)\")\n",
        "\n",
        "# 9) Dummy forward + custom loss + backward to confirm gradients flow\n",
        "prompt  = \"Test gradient flow\"\n",
        "inputs  = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "logits  = model(**inputs).logits  # [B, L, V] in FP16\n",
        "shifted = logits[..., :-1, :].reshape(-1, logits.size(-1))\n",
        "labels  = inputs[\"input_ids\"][..., 1:].reshape(-1)\n",
        "\n",
        "loss = torch.nn.functional.cross_entropy(shifted, labels)\n",
        "print(\"loss.requires_grad?\", loss.requires_grad)  # should be True\n",
        "\n",
        "loss.backward()\n",
        "print(\"Backward Success\")"
      ],
      "metadata": {
        "id": "XPPkbGDIdqNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Dataset"
      ],
      "metadata": {
        "id": "wUn8Dyn6wMBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class JSONLDataset(Dataset):\n",
        "    def __init__(self, filepath):\n",
        "        self.samples = []\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f:\n",
        "                self.samples.append(json.loads(line))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        '''Only if custom implementation works'''\n",
        "        sample = self.samples[idx]\n",
        "        prompt = sample['prompt']\n",
        "        chosen = prompt + sample['chosen_response']\n",
        "        rejected = prompt + sample['rejected_response']\n",
        "\n",
        "        score_keys = ['accuracy', 'safety', 'explanation_depth']\n",
        "        chosen_scores = torch.tensor([sample['chosen_scores'][k] for k in score_keys], dtype=torch.float)\n",
        "        rejected_scores = torch.tensor([sample['rejected_scores'][k] for k in score_keys], dtype=torch.float)\n",
        "        return {\n",
        "            'prompt_chosen_response': chosen,\n",
        "            'prompt_rejected_response': rejected,\n",
        "            'chosen_scores': chosen_scores,\n",
        "            'rejected_scores': rejected_scores,\n",
        "        }"
      ],
      "metadata": {
        "id": "q5GDJz4rhCpo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_data = JSONLDataset('/content/drive/MyDrive/DPO/DPO on Colab/gemma3_dpo_scored_data.jsonl')\n",
        "train_dataloader = DataLoader(train_data, batch_size = 1, shuffle = True)\n",
        "\n",
        "for sample in train_dataloader:\n",
        "  print(sample)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAZm90CsfuFc",
        "outputId": "a4094702-0706-47a3-957b-8dd3230fe606"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt_chosen_response': ['Question: A 44-year-old man comes to the physician for a follow-up examination. Ten months ago, he was diagnosed with HIV infection and appropriate antiretroviral therapy was initiated. Physical examination shows no abnormalities. Laboratory studies show increased viral load despite ongoing treatment. His pharmacotherapy is switched to a new combination drug regimen including an agent that binds to glycoprotein 41. The expected effect of this drug is most likely due to inhibition of which of the following?\\n\\nOptions:\\nA. Viral particle assembly\\nB. Viral docking and attachment to host cells\\nC. Viral genome transcription\\nD. Viral fusion and entry into host cells\\n\\nChoose the best answer and provide a step-by-step explanation for your choice.D. Viral fusion and entry into host cells\\nExplanation: The agent that binds to glycoprotein 41 inhibits viral fusion and entry into host cells, preventing HIV from successfully infecting new cells, which is crucial in managing increased viral load.'], 'prompt_rejected_response': [\"Question: A 44-year-old man comes to the physician for a follow-up examination. Ten months ago, he was diagnosed with HIV infection and appropriate antiretroviral therapy was initiated. Physical examination shows no abnormalities. Laboratory studies show increased viral load despite ongoing treatment. His pharmacotherapy is switched to a new combination drug regimen including an agent that binds to glycoprotein 41. The expected effect of this drug is most likely due to inhibition of which of the following?\\n\\nOptions:\\nA. Viral particle assembly\\nB. Viral docking and attachment to host cells\\nC. Viral genome transcription\\nD. Viral fusion and entry into host cells\\n\\nChoose the best answer and provide a step-by-step explanation for your choice.C. Viral genome transcription\\nExplanation: The drug binds to glycoprotein 41, which is crucial for viral assembly, and blocks the interaction between the virus and the host cell, thereby reducing viral particle assembly.\\n\\nHere's a step-by-step explanation:\\n\\n1. **Understanding the Problem:** The patient has HIV infection and a high viral load despite antiretroviral therapy. This suggests the virus is still actively replicating and assembling.\\n2. **New Drug Mechanism:** The new drug (containing a glycoprotein binding agent) is designed to inhibit viral particle assembly, thereby reducing the amount of viral particles that are actively assembling.\\n\"], 'chosen_scores': tensor([[5., 5., 4.]]), 'rejected_scores': tensor([[2., 3., 2.]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex = train_data[0]\n",
        "\n",
        "print(ex)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpSsiH1hcfIZ",
        "outputId": "9704af4a-9ba9-4388-8c52-b6dfb7336d06"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt_chosen_response': \"Question: A 5-year-old girl is brought to the physician because of a 2-day history of redness and foreign body sensation in both eyes. She has not had vision loss. Her mother reports that she has also had violent coughing spells followed by a high-pitched inspiratory sound during this time. For the past week, she has had low-grade fevers and a runny nose. Her only vaccinations were received at birth. Her temperature is 37.7°C (99.9°F). Examination shows conjunctival hemorrhage and petechiae. Oropharyngeal examination shows no abnormalities. Which of the following is the most appropriate pharmacotherapy?\\n\\nOptions:\\nA. Topical azithromycin\\nB. Oral azithromycin\\nC. Artificial tears\\nD. Topical tobramycin\\n\\nChoose the best answer and provide a step-by-step explanation for your choice.B. Oral azithromycin\\nExplanation: The child's symptoms, including conjunctival hemorrhage and cough with stridor, suggest a viral infection like measles, for which oral azithromycin is appropriate to prevent secondary bacterial infections.\", 'prompt_rejected_response': \"Question: A 5-year-old girl is brought to the physician because of a 2-day history of redness and foreign body sensation in both eyes. She has not had vision loss. Her mother reports that she has also had violent coughing spells followed by a high-pitched inspiratory sound during this time. For the past week, she has had low-grade fevers and a runny nose. Her only vaccinations were received at birth. Her temperature is 37.7°C (99.9°F). Examination shows conjunctival hemorrhage and petechiae. Oropharyngeal examination shows no abnormalities. Which of the following is the most appropriate pharmacotherapy?\\n\\nOptions:\\nA. Topical azithromycin\\nB. Oral azithromycin\\nC. Artificial tears\\nD. Topical tobramycin\\n\\nChoose the best answer and provide a step-by-step explanation for your choice.B. Oral azithromycin\\nExplanation: The patient presents with conjunctival hemorrhage and petechiae, which suggest systemic infection, and the history of violent coughing suggests possible infection. Oral azithromycin is a suitable option to treat systemic infections.\\n\\nLet's analyze the options:\\n\\n*   **A. Topical azithromycin:** Topical azithromycin is not generally indicated for systemic infections.\\n*   **B. Oral azithromycin:** Oral azithromycin is a good choice for treating systemic infections, including conjunctivitis.\\n*   **C. Artificial tears:** Artificial tears can provide temporary relief but are not a primary\", 'chosen_scores': tensor([2., 3., 2.]), 'rejected_scores': tensor([4., 5., 4.])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Loop"
      ],
      "metadata": {
        "id": "dm9Ww53LiFKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "importlib.reload(med_dpo_loss)\n",
        "from med_dpo_loss import MedDPOLoss\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "dtype = next(model.parameters()).dtype\n",
        "device = model.device\n",
        "\n",
        "print(device)\n",
        "#print(dtype)\n",
        "\n",
        "\n",
        "def train(model, tokenizer, dataloader, optimizer, epochs = 1):\n",
        "    model.train()\n",
        "\n",
        "    loss_fn = MedDPOLoss()\n",
        "\n",
        "    losses_50 = []\n",
        "\n",
        "    total_loss = 0.0\n",
        "    i = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "      for batch in dataloader:\n",
        "        i+=1\n",
        "        if i % 50 == 0:\n",
        "          print(f\"through {i} pairs: loss = {total_loss / i}\")\n",
        "          losses_50.append(total_loss / i)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        chosen_inputs = tokenizer(batch['prompt_chosen_response'], return_tensors='pt',\n",
        "                                  padding = True,\n",
        "                                  truncation = True,\n",
        "                                  max_length = 2048).to(device = 'cuda')\n",
        "        chosen_inputs = {key: value.to(device='cuda', dtype=torch.long) for key, value in chosen_inputs.items()}\n",
        "        chosen_rewards = batch['chosen_scores']\n",
        "\n",
        "        rejected_inputs = tokenizer(batch['prompt_rejected_response'], return_tensors='pt',\n",
        "                                    padding = True,\n",
        "                                    truncation = True,\n",
        "                                    max_length = 2048).to(device = 'cuda')\n",
        "        rejected_rewards = batch['rejected_scores']\n",
        "\n",
        "        chosen_outputs = model(**chosen_inputs)\n",
        "        #print(chosen_outputs.logits)\n",
        "\n",
        "        rejected_outputs = model(**rejected_inputs)\n",
        "        #print(rejected_outputs.logits)\n",
        "\n",
        "        chosen_logits = chosen_outputs.logits\n",
        "        rejected_logits = rejected_outputs.logits\n",
        "\n",
        "        chosen_rewards = chosen_rewards.to(device)\n",
        "        rejected_rewards = rejected_rewards.to(device)\n",
        "\n",
        "        per_examples_loss = loss_fn(chosen_logits, rejected_logits,\n",
        "                       chosen_rewards, rejected_rewards)\n",
        "\n",
        "        loss = per_examples_loss.mean()\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_postfix(batch=i, loss=loss.item(), refresh=False)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "      losses_50.append(total_loss / i)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "\n",
        "train(model, tokenizer, train_dataloader, optimizer)"
      ],
      "metadata": {
        "id": "n1iPLjwFiGqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference, Load Model if needed\n"
      ],
      "metadata": {
        "id": "ciMxsdtyXgnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# 1) Re–build the exact same 4-bit / FP16 base you used for training\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# 2) Wrap it in PEFT by loading your DPO adapter\n",
        "#    If you called `model.save_adapter(..., adapter_name=\"new_task\")`, then:\n",
        "model = PeftModel.from_pretrained(\n",
        "    base,\n",
        "    \"/content/drive/MyDrive/DPO/DPO on Colab/gemma_med_dpo_adapter\",  # path where you saved it\n",
        "    adapter_name=\"new_task\",                           # the name you used\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# 3) Load (or re‐load) your tokenizer from the adapter folder,\n",
        "#    so you get any special tokens / settings you used\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"/content/drive/MyDrive/DPO/DPO on Colab/gemma_med_dpo_adapter\",\n",
        "    use_fast=True,\n",
        ")\n",
        "\n",
        "# 4) Switch to eval mode and generate\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "EnVaEO16p3Uz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "00270926-9743-4d9d-c0c2-eb2708fd5b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-451646d41d4d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 1) Rebuild the same 4-bit + FP16 compute config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackbone_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackboneConfigMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBackboneMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mchat_template_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocstringParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeHintParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_json_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMAGENET_DEFAULT_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_DEFAULT_STD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_STD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m from .doc import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/chat_template_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Test Responses"
      ],
      "metadata": {
        "id": "0F-nyXsimL5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def perform_inference(model, tokenizer, prompts, batch_size=16, max_new_tokens=128):\n",
        "    model.to(device=\"cuda\")\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
        "        batch_prompts = prompts[i:i + batch_size]\n",
        "\n",
        "        # Tokenize and pad to longest sequence in batch\n",
        "        inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,  # deterministic generation (change if you want randomness)\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "        decoded_outputs = tokenizer.batch_decode(outputs[:, -max_new_tokens:], skip_special_tokens=True)\n",
        "\n",
        "        for prompt, response in zip(batch_prompts, decoded_outputs):\n",
        "            results.append({\"prompt\": prompt, \"response\": response})\n",
        "\n",
        "    return results\n",
        "\n",
        "def load_prompts_from_jsonl(file_path):\n",
        "    prompts = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                data = json.loads(line)\n",
        "                prompts.append(data['prompt'])\n",
        "    return prompts\n",
        "\n",
        "def save_results_to_json(results, output_file):\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "model_name = \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\"\n",
        "prompts_file = \"/content/drive/MyDrive/DPO/DPO on Colab/gemma3_sft_test_results.jsonl\"\n",
        "output_file = \"/content/drive/MyDrive/DPO/DPO on Colab/gemma3_dpo_inference_results.json\"\n",
        "\n",
        "prompts = load_prompts_from_jsonl(prompts_file)\n",
        "results = perform_inference(model, tokenizer, prompts)\n",
        "#save_results_to_json(results, output_file)"
      ],
      "metadata": {
        "id": "CPTiowqxXmM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"Once upon a time\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    out = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93cF8jlLdPeV",
        "outputId": "1c0fea2e-8e38-4187-f60c-995aec7f02da"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, in a world filled with shimmering rivers and towering trees, lived a tiny firefly named Flicker. He was incredibly curious, and his light, a faint and flickering orange glow, was barely enough to illuminate his own little patch of forest.\n",
            "\n",
            "The\n"
          ]
        }
      ]
    }
  ]
}