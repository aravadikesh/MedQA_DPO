

@misc{ekinci-2025,
	author = {Ekinci, Bugra},
	month = {1},
	title = {{The Power of small LLMs in Healthcare: A RAG Framework Alternative to large language models - John Snow Labs}},
	year = {2025},
	url = {https://www.johnsnowlabs.com/the-power-of-small-llms-in-healthcare-a-rag-framework-alternative-to-large-language-models/},
}

@misc{nori2023capabilitiesgpt4medicalchallenge,
      title={Capabilities of GPT-4 on Medical Challenge Problems}, 
      author={Harsha Nori and Nicholas King and Scott Mayer McKinney and Dean Carignan and Eric Horvitz},
      year={2023},
      eprint={2303.13375},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.13375}, 
}

@misc{bai2024efficiencysystematicsurveyresourceefficient,
      title={Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models}, 
      author={Guangji Bai and Zheng Chai and Chen Ling and Shiyu Wang and Jiaying Lu and Nan Zhang and Tingwei Shi and Ziyang Yu and Mengdan Zhu and Yifei Zhang and Xinyuan Song and Carl Yang and Yue Cheng and Liang Zhao},
      year={2024},
      eprint={2401.00625},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.00625}, 
}

@misc{tran2024bioinstructinstructiontuninglarge,
      title={BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing}, 
      author={Hieu Tran and Zhichao Yang and Zonghai Yao and Hong Yu},
      year={2024},
      eprint={2310.19975},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.19975}, 
}

@misc{ouyang-2022,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.02155}, 
}

@misc{bai2022constitutionalaiharmlessnessai,
      title={Constitutional AI: Harmlessness from AI Feedback}, 
      author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
      year={2022},
      eprint={2212.08073},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.08073}, 
}

@misc{luo2025empiricalstudycatastrophicforgetting,
      title={An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning}, 
      author={Yun Luo and Zhen Yang and Fandong Meng and Yafu Li and Jie Zhou and Yue Zhang},
      year={2025},
      eprint={2308.08747},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.08747}, 
}

@misc{ren2024analyzingreducingcatastrophicforgetting,
      title={Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning}, 
      author={Weijieying Ren and Xinlong Li and Lei Wang and Tianxiang Zhao and Wei Qin},
      year={2024},
      eprint={2402.18865},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.18865}, 
}

@misc{labrak2024biomistralcollectionopensourcepretrained,
      title={BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains}, 
      author={Yanis Labrak and Adrien Bazoge and Emmanuel Morin and Pierre-Antoine Gourraud and Mickael Rouvier and Richard Dufour},
      year={2024},
      eprint={2402.10373},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.10373}, 
}

@misc{han2023medalpacaopensourcecollection,
      title={MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data}, 
      author={Tianyu Han and Lisa C. Adams and Jens-Michalis Papaioannou and Paul Grundmann and Tom Oberhauser and Alexander LÃ¶ser and Daniel Truhn and Keno K. Bressem},
      year={2023},
      eprint={2304.08247},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.08247}, 
}

@inproceedings{khlaut-etal-2024-efficient,
    title = "Efficient Medical Question Answering with Knowledge-Augmented Question Generation",
    author = "Khlaut, Julien  and
      Dancette, Corentin  and
      Ferreres, Elodie  and
      Alaedine, Benani  and
      Herent, Herent  and
      Manceron, Pierre",
    editor = "Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Bitterman, Danielle",
    booktitle = "Proceedings of the 6th Clinical Natural Language Processing Workshop",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.clinicalnlp-1.2/",
    doi = "10.18653/v1/2024.clinicalnlp-1.2",
    pages = "10--20",
    abstract = "In the expanding field of language model applications, medical knowledge representation remains a significant challenge due to the specialized nature of the domain. Large language models, such as GPT-4, obtain reasonable scores on medical question-answering tasks, but smaller models are far behind.In this work, we introduce a method to improve the proficiency of a small language model in the medical domain by employing a two-fold approach. We first fine-tune the model on a corpus of medical textbooks. Then, we use GPT-4 to generate questions similar to the downstream task, prompted with textbook knowledge, and use them to fine-tune the model. Additionally, we introduce ECN-QA, a novel Medical QA dataset containing {\textquotedblleft}progressive questions{\textquotedblright} composed of related sequential questions. We show the benefits of our training strategy on this dataset.The study`s findings highlight the potential of small language models in the medical domain when appropriately fine-tuned."
}

@inproceedings{yang-etal-2024-direct,
    title = "Direct Preference Optimization for Neural Machine Translation with Minimum {B}ayes Risk Decoding",
    author = "Yang, Guangyu  and
      Chen, Jinghong  and
      Lin, Weizhe  and
      Byrne, Bill",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-short.34/",
    doi = "10.18653/v1/2024.naacl-short.34",
    pages = "391--398",
    abstract = "Minimum Bayes Risk (MBR) decoding can significantly improve translation performance of Multilingual Large Language Models (MLLMs). However, MBR decoding is computationally expensive. We show how the recently developed Reinforcement Learning technique, Direct Preference Optimization (DPO), can fine-tune MLLMs to get the gains of MBR without any additional computation in inference. Our method uses only a small monolingual fine-tuning set and yields significantly improved performance on multiple NMT test sets compared to MLLMs without DPO."
}

@inproceedings{chen-etal-2024-exploring-potential,
    title = "Exploring the Potential of Large Language Models in Computational Argumentation",
    author = "Chen, Guizhen  and
      Cheng, Liying  and
      Luu, Anh Tuan  and
      Bing, Lidong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.126/",
    doi = "10.18653/v1/2024.acl-long.126",
    pages = "2309--2330",
    abstract = "Computational argumentation has become an essential tool in various domains, including law, public policy, and artificial intelligence. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated impressive capabilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on diverse computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models, and LLaMA2 models, in both zero-shot and few-shot settings. We organize existing tasks into six main categories and standardize the format of fourteen openly available datasets. In addition, we present a new benchmark dataset on counter speech generation that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of the datasets, demonstrating their capabilities in the field of argumentation. Our analysis offers valuable suggestions for evaluating computational argumentation and its integration with LLMs in future research endeavors."
}

@misc{liu2023reviewreinforcementlearningnatural,
      title={A Review of Reinforcement Learning for Natural Language Processing, and Applications in Healthcare}, 
      author={Ying Liu and Haozhu Wang and Huixue Zhou and Mingchen Li and Yu Hou and Sicheng Zhou and Fang Wang and Rama Hoetzlein and Rui Zhang},
      year={2023},
      eprint={2310.18354},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.18354}, 
}

@inproceedings{zhang-etal-2023-ds4dh,
    title = "{DS}4{DH} at {MEDIQA}-Chat 2023: Leveraging {SVM} and {GPT}-3 Prompt Engineering for Medical Dialogue Classification and Summarization",
    author = "Zhang, Boya  and
      Mishra, Rahul  and
      Teodoro, Douglas",
    editor = "Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 5th Clinical Natural Language Processing Workshop",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.clinicalnlp-1.57/",
    doi = "10.18653/v1/2023.clinicalnlp-1.57",
    pages = "536--545",
    abstract = "This paper presents the results of the Data Science for Digital Health (DS4DH) group in the MEDIQA-Chat Tasks at ACL-ClinicalNLP 2023. Our study combines the power of a classical machine learning method, Support Vector Machine, for classifying medical dialogues, along with the implementation of one-shot prompts using GPT-3.5. We employ dialogues and summaries from the same category as prompts to generate summaries for novel dialogues. Our findings exceed the average benchmark score, offering a robust reference for assessing performance in this field."
}

@inproceedings{wang-etal-2023-fine,
    title = "Fine-grained Medical Vision-Language Representation Learning for Radiology Report Generation",
    author = "Wang, Siyuan  and
      Peng, Bo  and
      Liu, Yichao  and
      Peng, Qi",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.989/",
    doi = "10.18653/v1/2023.emnlp-main.989",
    pages = "15949--15956",
    abstract = "Given the input radiology images, the objective of radiology report generation is to produce accurate and comprehensive medical reports, which typically include multiple descriptive clinical sentences associated with different phenotypes. Most existing works have relied on a pre-trained vision encoder to extract the visual representations of the images. In this study, we propose a phenotype-driven medical vision-language representation learning framework to efficiently bridge the gap between visual and textual modalities for improved text-oriented generation. In contrast to conventional methods which learn medical vision-language representations by contrasting images with entire reports, our approach learns more fine-grained representations by contrasting images with each sentence within the reports. The learned fine-grained representations can be used to improve radiology report generation. The experiments on two widely-used datasets MIMIC-CXR and IU X-ray demonstrate that our method can achieve promising performances and substantially outperform the conventional vision-language representation learning methods."
}

@misc{yang2025llmmedqaenhancingmedicalquestion,
      title={LLM-MedQA: Enhancing Medical Question Answering through Case Studies in Large Language Models}, 
      author={Hang Yang and Hao Chen and Hui Guo and Yineng Chen and Ching-Sheng Lin and Shu Hu and Jinrong Hu and Xi Wu and Xin Wang},
      year={2025},
      eprint={2501.05464},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.05464}, 
}