{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets openai backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import openai\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import backoff\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"generation.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 09:59:30,468 - INFO - Configuration:\n",
      "2025-04-28 09:59:30,469 - INFO -   Explanation Model: gpt-4o-mini\n",
      "2025-04-28 09:59:30,470 - INFO -   Dataset: GBaker/MedQA-USMLE-4-options\n",
      "2025-04-28 09:59:30,471 - INFO -   Splits: ['train']\n",
      "2025-04-28 09:59:30,472 - INFO -   Output Directory: synthetic_medqa_data2\n",
      "2025-04-28 09:59:30,473 - INFO -   Max Samples Per Split: None\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "SEED = 42\n",
    "EXPLANATION_MODEL = \"gpt-4o-mini\"  \n",
    "DATASET_NAME = \"GBaker/MedQA-USMLE-4-options\"\n",
    "SPLITS_TO_PROCESS = [\"train\"] \n",
    "OUTPUT_DIR = \"synthetic_medqa_data2\"\n",
    "MAX_SAMPLES_PER_SPLIT = None # Set to a number (e.g., 100) for testing, None to process all\n",
    "\n",
    "CHECKPOINT_INTERVAL = 10  # Save checkpoint every 10 samples \n",
    "BACKOFF_MAX_TRIES = 8    # Maximum number of retries for rate limits\n",
    "BACKOFF_FACTOR = 2       # Exponential backoff factor\n",
    "\n",
    "# --- Output Format Configuration ---\n",
    "PROMPT_FORMAT = \"\"\"Question: {question}\n",
    "\n",
    "Options:\n",
    "{options_formatted}\n",
    "\n",
    "Choose the best answer and provide a step-by-step explanation for your choice.\"\"\"\n",
    "\n",
    "# How to format the 'chosen' and 'rejected' responses\n",
    "RESPONSE_FORMAT = \"\"\"{answer_label}. {answer_text}\n",
    "Explanation: {explanation}\"\"\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Configuration:\")\n",
    "logger.info(f\"  Explanation Model: {EXPLANATION_MODEL}\")\n",
    "logger.info(f\"  Dataset: {DATASET_NAME}\")\n",
    "logger.info(f\"  Splits: {SPLITS_TO_PROCESS}\")\n",
    "logger.info(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "logger.info(f\"  Max Samples Per Split: {MAX_SAMPLES_PER_SPLIT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferencePairGenerator:\n",
    "    def __init__(self, api_key: str, seed: int,\n",
    "                 explanation_model: str,\n",
    "                 prompt_format: str, response_format: str):\n",
    "        self.client = openai.OpenAI(api_key=api_key)\n",
    "        random.seed(seed)\n",
    "        self.explanation_model = explanation_model\n",
    "        self.prompt_format = prompt_format\n",
    "        self.response_format = response_format\n",
    "        self.label_to_index = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "        self.index_to_label = {v: k for k, v in self.label_to_index.items()}\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo, \n",
    "        (openai.RateLimitError, openai.APIConnectionError),\n",
    "        max_tries=BACKOFF_MAX_TRIES,  # Maximum number of retries\n",
    "        factor=BACKOFF_FACTOR,     # Exponential backoff factor\n",
    "        jitter=None   # Add randomness to backoff delays\n",
    "    )\n",
    "    def call_openai_api(self, messages: List[Dict[str, str]], model: str,\n",
    "                        temperature: float = 0.5, max_tokens: int = 500, timeout=60) -> Any:\n",
    "        \"\"\"Call OpenAI API with automatic backoff for rate limits\"\"\"\n",
    "        try:\n",
    "            return self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                timeout=timeout\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Handle non-rate-limit errors that backoff doesn't catch\n",
    "            if not isinstance(e, (openai.RateLimitError, openai.APIConnectionError)):\n",
    "                logger.error(f\"API Error (non-rate-limit): {e}\")\n",
    "            raise  # Re-raise to let backoff handle it if appropriate\n",
    "\n",
    "    def get_explanation(self, question: str, options_formatted: str, answer: str, is_correct: bool) -> str:\n",
    "        if is_correct:\n",
    "            prompt_detail = (\n",
    "                \"Generate a concise explanation (~20-30 words) highlighting the key clinical reasoning \"\n",
    "                \"and evidence from the vignette that justifies why this answer is the most correct choice.\"\n",
    "            )\n",
    "        else:\n",
    "            prompt_detail = (\n",
    "                \"Pretend this option is correct. Generate a concise explanation (~20-30 words) that appears \"\n",
    "                \"plausible but subtly contains flawed reasoning. Focus on relevant clinical details while \"\n",
    "                \"avoiding any direct mention that this choice might be incorrect.\"\n",
    "            )\n",
    "\n",
    "        prompt = (\n",
    "            f\"Medical Question Context:\\n{question}\\n\\n\"\n",
    "            f\"Options:\\n{options_formatted}\\n\\n\"\n",
    "            f\"Answer Choice to Explain: {answer}\\n\\n\"\n",
    "            f\"{prompt_detail}\"\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a highly knowledgeable medical expert specializing in clinical reasoning \"\n",
    "                    \"explanations for USMLE-style questions. Be clear, concise, and follow instructions carefully.\"\n",
    "                )\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            response = self.call_openai_api(\n",
    "                messages=messages,\n",
    "                model=self.explanation_model,\n",
    "                temperature=0.6,\n",
    "                max_tokens=150\n",
    "            )\n",
    "\n",
    "            if response:\n",
    "                explanation = response.choices[0].message.content.strip()\n",
    "                if not explanation or any(phrase in explanation.lower() for phrase in [\"cannot provide\", \"explanation could not be generated\"]):\n",
    "                    logger.warning(\"Received potentially invalid explanation. Falling back.\")\n",
    "                    return \"[Fallback] Explanation generation failed or returned invalid content.\"\n",
    "                return explanation\n",
    "            else:\n",
    "                return \"[API Error] Explanation could not be generated due to API failure.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating explanation: {e}\")\n",
    "            return \"[Error] An explanation could not be generated.\"\n",
    "\n",
    "    def select_alternative_answer(self, options: List[str], correct_index: int) -> int:\n",
    "        incorrect_indices = [i for i in range(len(options)) if i != correct_index]\n",
    "        return random.choice(incorrect_indices) if incorrect_indices else (correct_index + 1) % len(options)\n",
    "\n",
    "    def process_sample(self, sample: Dict[str, Any], idx: int) -> Optional[Dict[str, Any]]:\n",
    "        try:\n",
    "            question = sample.get(\"question\")\n",
    "            options = sample.get(\"options\")\n",
    "\n",
    "            if not question or not options:\n",
    "                logger.warning(f\"Sample {idx}: Missing question or options. Skipping.\")\n",
    "                return None\n",
    "\n",
    "            correct_label = sample.get(\"answer_idx\").upper()\n",
    "            correct_index = self.label_to_index[correct_label]\n",
    "            correct_option_text = options[correct_label]\n",
    "\n",
    "            options_formatted = \"\\n\".join(\n",
    "                [f\"{self.index_to_label[i]}. {opt}\" for i, opt in enumerate(options.values())]\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Sample {idx}: Generating explanation for CORRECT answer ({correct_label})\")\n",
    "            correct_explanation = self.get_explanation(question, options_formatted, correct_option_text, is_correct=True)\n",
    "            if \"[Error]\" in correct_explanation or \"[API Error]\" in correct_explanation:\n",
    "                logger.error(f\"Sample {idx}: Failed to generate explanation for correct answer. Skipping.\")\n",
    "                return None\n",
    "\n",
    "            alt_index = self.select_alternative_answer(list(options.values()), correct_index)\n",
    "            alt_label = self.index_to_label[alt_index]\n",
    "            alt_option_text = list(options.values())[alt_index]\n",
    "\n",
    "            logger.info(f\"Sample {idx}: Generating explanation for ALTERNATIVE answer ({alt_label})\")\n",
    "            alt_explanation = self.get_explanation(question, options_formatted, alt_option_text, is_correct=False)\n",
    "\n",
    "            prompt_str = self.prompt_format.format(question=question, options_formatted=options_formatted)\n",
    "            chosen_str = self.response_format.format(answer_label=correct_label, answer_text=correct_option_text, explanation=correct_explanation)\n",
    "            rejected_str = self.response_format.format(answer_label=alt_label, answer_text=alt_option_text, explanation=alt_explanation)\n",
    "\n",
    "            return {\n",
    "                \"prompt\": prompt_str,\n",
    "                \"chosen\": chosen_str,\n",
    "                \"rejected\": rejected_str,\n",
    "                \"metadata\": {\n",
    "                    \"original_question\": question,\n",
    "                    \"options\": options,\n",
    "                    \"correct_index\": correct_index,\n",
    "                    \"alternative_index\": alt_index,\n",
    "                    \"correct_label\": correct_label,\n",
    "                    \"alternative_label\": alt_label,\n",
    "                    \"correct_explanation_raw\": correct_explanation,\n",
    "                    \"alternative_explanation_raw\": alt_explanation,\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Critical error processing sample {idx}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def process_dataset(self, dataset_name: str, splits: List[str],\n",
    "                    output_dir: str, max_samples: Optional[int] = None,\n",
    "                    checkpoint_interval: int = 5) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:\n",
    "        \"\"\"\n",
    "        Load dataset splits, generate SFT and DPO data, and save results.\n",
    "        Results are also returned in a dict for optional in-code use.\n",
    "        \n",
    "        Args:\n",
    "            dataset_name: The HuggingFace dataset name\n",
    "            splits: List of dataset splits to process\n",
    "            output_dir: Directory to save output files\n",
    "            max_samples: Maximum number of samples to process per split\n",
    "            checkpoint_interval: How often to save checkpoint state\n",
    "            \n",
    "        Returns:\n",
    "            A dict mapping each split to its SFT and DPO data.\n",
    "        \"\"\"\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create checkpoint file for recovery\n",
    "        checkpoint_file = os.path.join(output_dir, \"checkpoint.json\")\n",
    "        \n",
    "        # Try to load checkpoint\n",
    "        checkpoint_data = {}\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            try:\n",
    "                with open(checkpoint_file, 'r') as f:\n",
    "                    checkpoint_data = json.load(f)\n",
    "                    logger.info(f\"Loaded checkpoint: {checkpoint_data}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load checkpoint: {e}\")\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for split in splits:\n",
    "            logger.info(f\"--- Processing {split} split ---\")\n",
    "            \n",
    "            # Skip if this split is already completed according to checkpoint\n",
    "            if checkpoint_data.get(f\"{split}_completed\", False):\n",
    "                logger.info(f\"Split {split} already completed according to checkpoint. Skipping.\")\n",
    "                \n",
    "                # Try to load the previously generated data for this split\n",
    "                try:\n",
    "                    sft_jsonl_path = os.path.join(output_dir, f\"sft_data_{split}.jsonl\")\n",
    "                    dpo_jsonl_path = os.path.join(output_dir, f\"dpo_data_{split}.jsonl\")\n",
    "                    \n",
    "                    sft_data = []\n",
    "                    with open(sft_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        for line in f:\n",
    "                            sft_data.append(json.loads(line))\n",
    "                    \n",
    "                    dpo_data = []\n",
    "                    with open(dpo_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        for line in f:\n",
    "                            dpo_data.append(json.loads(line))\n",
    "                    \n",
    "                    all_results[split] = {\n",
    "                        \"sft\": sft_data,\n",
    "                        \"dpo\": dpo_data\n",
    "                    }\n",
    "                    \n",
    "                    logger.info(f\"Loaded {len(sft_data)} SFT and {len(dpo_data)} DPO entries for {split} from files.\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to load previous data for completed split {split}: {e}\")\n",
    "                \n",
    "                continue\n",
    "                \n",
    "            # Get the last processed index for this split\n",
    "            last_idx = checkpoint_data.get(f\"{split}_last_idx\", -1)\n",
    "            \n",
    "            try:\n",
    "                # Load dataset\n",
    "                dataset = load_dataset(dataset_name, split=split)\n",
    "                logger.info(f\"Loaded dataset for {split} split.\")\n",
    "                \n",
    "                # Exclude datapoints over 1024 characters\n",
    "                dataset = dataset.filter(lambda x: len(x[\"question\"]) <= 1024)\n",
    "                \n",
    "                total_samples = len(dataset)\n",
    "                logger.info(f\"Total samples in {split}: {total_samples}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load dataset {dataset_name} for split {split}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare lists and file paths\n",
    "            sft_data = []\n",
    "            dpo_data = []\n",
    "            sft_jsonl_path = os.path.join(output_dir, f\"sft_data_{split}.jsonl\")\n",
    "            dpo_jsonl_path = os.path.join(output_dir, f\"dpo_data_{split}.jsonl\")\n",
    "            \n",
    "            # Handle resuming from checkpoint\n",
    "            start_idx = last_idx + 1\n",
    "            processed_count = 0\n",
    "            num_to_process = total_samples if max_samples is None else min(max_samples, total_samples)\n",
    "            \n",
    "            # Adjust pbar to reflect remaining work\n",
    "            pbar_desc = f\"Generating {split} data\" + (f\" (max {max_samples})\" if max_samples else \"\")\n",
    "            remaining_count = num_to_process - start_idx if start_idx > 0 else num_to_process\n",
    "            pbar_total = None if num_to_process is None else remaining_count\n",
    "            \n",
    "            # Check if files already exist and need to be appended to\n",
    "            append_mode = \"a\" if start_idx > 0 else \"w\"\n",
    "            \n",
    "            # Load existing data if continuing\n",
    "            if append_mode == \"a\":\n",
    "                try:\n",
    "                    if os.path.exists(sft_jsonl_path):\n",
    "                        with open(sft_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            for line in f:\n",
    "                                sft_data.append(json.loads(line))\n",
    "                    \n",
    "                    if os.path.exists(dpo_jsonl_path):\n",
    "                        with open(dpo_jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                            for line in f:\n",
    "                                dpo_data.append(json.loads(line))\n",
    "                    \n",
    "                    logger.info(f\"Loaded {len(sft_data)} existing SFT and {len(dpo_data)} DPO entries.\")\n",
    "                    processed_count = len(sft_data)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error loading existing data files: {e}\")\n",
    "            \n",
    "            # Open files once, then write results line by line inside the loop\n",
    "            try:\n",
    "                with open(sft_jsonl_path, append_mode, encoding=\"utf-8\") as f_sft, \\\n",
    "                    open(dpo_jsonl_path, append_mode, encoding=\"utf-8\") as f_dpo:\n",
    "                    \n",
    "                    # Create a mini-batch processing approach to avoid exhausting resources\n",
    "                    batch_size = 10  # Process 10 samples at a time\n",
    "                    \n",
    "                    # Only iterate over samples we haven't processed yet\n",
    "                    dataset_slice = dataset.select(range(start_idx, len(dataset)))\n",
    "                    \n",
    "                    # Create progress bar for remaining items\n",
    "                    pbar = tqdm(enumerate(dataset_slice, start=start_idx), \n",
    "                            total=pbar_total, \n",
    "                            desc=pbar_desc)\n",
    "                    \n",
    "                    for idx, sample in pbar:\n",
    "                        # Check if we've reached the limit\n",
    "                        if max_samples is not None and processed_count >= max_samples:\n",
    "                            logger.info(f\"Reached max_samples limit ({max_samples}) for {split} split.\")\n",
    "                            break\n",
    "                        \n",
    "                        # Actual processing with error handling\n",
    "                        try:\n",
    "                            processed_result = self.process_sample(sample, idx)\n",
    "                            if processed_result:\n",
    "                                # Create the SFT item\n",
    "                                sft_item = {\n",
    "                                    \"prompt\": processed_result[\"prompt\"],\n",
    "                                    \"response\": processed_result[\"chosen\"]\n",
    "                                }\n",
    "                                # Create the DPO item\n",
    "                                dpo_item = {\n",
    "                                    \"prompt\": processed_result[\"prompt\"],\n",
    "                                    \"chosen\": processed_result[\"chosen\"],\n",
    "                                    \"rejected\": processed_result[\"rejected\"],\n",
    "                                    \"metadata\": processed_result[\"metadata\"]\n",
    "                                }\n",
    "                                \n",
    "                                # Immediately write SFT item\n",
    "                                f_sft.write(json.dumps(sft_item, ensure_ascii=False) + \"\\n\")\n",
    "                                f_sft.flush()\n",
    "                                # Immediately write DPO item\n",
    "                                f_dpo.write(json.dumps(dpo_item, ensure_ascii=False) + \"\\n\")\n",
    "                                f_dpo.flush()\n",
    "                                \n",
    "                                # Also store them in memory\n",
    "                                sft_data.append(sft_item)\n",
    "                                dpo_data.append(dpo_item)\n",
    "                                \n",
    "                                processed_count += 1\n",
    "                                \n",
    "                                # Update progress bar description\n",
    "                                pbar.set_description(f\"Processing {split} ({processed_count}/{num_to_process})\")\n",
    "                            else:\n",
    "                                logger.warning(f\"Sample {idx} skipped due to processing errors.\")\n",
    "                                \n",
    "                            # Save checkpoint at regular intervals\n",
    "                            if idx % checkpoint_interval == 0:\n",
    "                                checkpoint_data[f\"{split}_last_idx\"] = idx\n",
    "                                with open(checkpoint_file, 'w') as f:\n",
    "                                    json.dump(checkpoint_data, f)\n",
    "                                logger.info(f\"Saved checkpoint at index {idx}\")\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            logger.exception(f\"Error processing sample {idx}: {e}\")\n",
    "                            # Save checkpoint on error to enable recovery\n",
    "                            checkpoint_data[f\"{split}_last_idx\"] = idx - 1  # Mark the last successful one\n",
    "                            with open(checkpoint_file, 'w') as f:\n",
    "                                json.dump(checkpoint_data, f)\n",
    "                            # Don't break the loop - continue with next sample\n",
    "                    \n",
    "                    # Mark split as completed\n",
    "                    checkpoint_data[f\"{split}_completed\"] = True\n",
    "                    checkpoint_data[f\"{split}_last_idx\"] = idx\n",
    "                    with open(checkpoint_file, 'w') as f:\n",
    "                        json.dump(checkpoint_data, f)\n",
    "                    logger.info(f\"Marked split {split} as completed in checkpoint\")\n",
    "                    \n",
    "                    logger.info(f\"Finished processing {processed_count} samples for {split} split.\")\n",
    "                    logger.info(f\"Saved {len(sft_data)} SFT entries to {sft_jsonl_path}\")\n",
    "                    logger.info(f\"Saved {len(dpo_data)} DPO entries to {dpo_jsonl_path}\")\n",
    "                    \n",
    "                    # Optionally show a sample entry for sanity check\n",
    "                    if dpo_data:\n",
    "                        logger.info(f\"--- Sample {split} DPO entry ---\")\n",
    "                        logger.info(json.dumps(dpo_data[0], indent=2, ensure_ascii=False))\n",
    "                        logger.info(f\"--- Sample {split} SFT entry ---\")\n",
    "                        logger.info(json.dumps(sft_data[0], indent=2, ensure_ascii=False))\n",
    "                        \n",
    "            except IOError as e:\n",
    "                logger.error(f\"Failed to write data for {split} split: {e}\")\n",
    "            \n",
    "            # Store the results for this split in the master dictionary\n",
    "            all_results[split] = {\n",
    "                \"sft\": sft_data,\n",
    "                \"dpo\": dpo_data\n",
    "            }\n",
    "        \n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Instantiation and Execution\n",
    "generator = PreferencePairGenerator(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        seed=SEED,\n",
    "        explanation_model=EXPLANATION_MODEL,\n",
    "        prompt_format=PROMPT_FORMAT,\n",
    "        response_format=RESPONSE_FORMAT\n",
    ")\n",
    "    \n",
    "# Run the dataset processing\n",
    "results = generator.process_dataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    splits=SPLITS_TO_PROCESS,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_samples=MAX_SAMPLES_PER_SPLIT,\n",
    "    checkpoint_interval=CHECKPOINT_INTERVAL\n",
    ")\n",
    "\n",
    "# Summary\n",
    "total_sft_pairs = sum(len(data[\"sft\"]) for data in results.values())\n",
    "total_dpo_pairs = sum(len(data[\"dpo\"]) for data in results.values())\n",
    "logger.info(f\"\\n--- Generation Complete ---\")\n",
    "logger.info(f\"Processed splits: {list(results.keys())}\")\n",
    "logger.info(f\"Total SFT pairs generated: {total_sft_pairs}\")\n",
    "logger.info(f\"Total DPO pairs generated: {total_dpo_pairs}\")\n",
    "logger.info(f\"Data saved in directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Check for duplicate questions in the produced files\n",
    "\n",
    "def check_duplicates(output_dir: str, splits: List[str]) -> None:\n",
    "    for split in splits:\n",
    "        sft_file = os.path.join(output_dir, f\"sft_data_{split}.jsonl\")\n",
    "        if not os.path.exists(sft_file):\n",
    "            logger.warning(f\"File {sft_file} does not exist. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        questions = []\n",
    "        with open(sft_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                questions.append(data[\"prompt\"])  # Assuming \"prompt\" contains the question\n",
    "\n",
    "        # Count occurrences of each question\n",
    "        question_counts = Counter(questions)\n",
    "        duplicates = {q: count for q, count in question_counts.items() if count > 1}\n",
    "\n",
    "        if duplicates:\n",
    "            logger.info(f\"Found {len(duplicates)} duplicate questions in {split} split:\")\n",
    "            for question, count in duplicates.items():\n",
    "                logger.info(f\"  - {question[:100]}... (repeated {count} times)\")\n",
    "        else:\n",
    "            logger.info(f\"No duplicate questions found in {split} split.\")\n",
    "\n",
    "# Run the duplicate check\n",
    "check_duplicates(OUTPUT_DIR, SPLITS_TO_PROCESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = PreferencePairGenerator(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        seed=SEED,\n",
    "        explanation_model=EXPLANATION_MODEL,\n",
    "        prompt_format=PROMPT_FORMAT,\n",
    "        response_format=RESPONSE_FORMAT\n",
    ")\n",
    "\n",
    "# Process the test split\n",
    "test_results = generator.process_dataset(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    splits=[\"test\"],  # Specify the test split\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_samples=MAX_SAMPLES_PER_SPLIT,\n",
    "    checkpoint_interval=CHECKPOINT_INTERVAL\n",
    ")\n",
    "\n",
    "# Summary for the test split\n",
    "total_test_sft_pairs = len(test_results[\"test\"][\"sft\"]) if \"test\" in test_results else 0\n",
    "total_test_dpo_pairs = len(test_results[\"test\"][\"dpo\"]) if \"test\" in test_results else 0\n",
    "logger.info(f\"\\n--- Test Split Processing Complete ---\")\n",
    "logger.info(f\"Total SFT pairs generated for test split: {total_test_sft_pairs}\")\n",
    "logger.info(f\"Total DPO pairs generated for test split: {total_test_dpo_pairs}\")\n",
    "logger.info(f\"Test data saved in directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_length_metrics_from_file(file_path: str) -> Dict[str, Any]:\n",
    "    if not os.path.exists(file_path):\n",
    "        logger.warning(f\"File {file_path} does not exist.\")\n",
    "        return {}\n",
    "\n",
    "    prompts = []\n",
    "    responses = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            prompts.append(len(data[\"prompt\"]))\n",
    "            responses.append(len(data[\"response\"]))\n",
    "\n",
    "    metrics = {\n",
    "        \"prompt\": {\n",
    "            \"min\": np.min(prompts),\n",
    "            \"max\": np.max(prompts),\n",
    "            \"mean\": np.mean(prompts),\n",
    "            \"median\": np.median(prompts)\n",
    "        },\n",
    "        \"response\": {\n",
    "            \"min\": np.min(responses),\n",
    "            \"max\": np.max(responses),\n",
    "            \"mean\": np.mean(responses),\n",
    "            \"median\": np.median(responses)\n",
    "        }\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for the test split from file\n",
    "# train_sft_file = os.path.join(OUTPUT_DIR, \"sft_data_train.jsonl\")\n",
    "# test_sft_file = os.path.join(OUTPUT_DIR, \"sft_data_test.jsonl\")\n",
    "# metrics_from_file_train = calculate_length_metrics_from_file(test_sft_file)\n",
    "# metrics_from_file_test = calculate_length_metrics_from_file(train_sft_file)\n",
    "\n",
    "\n",
    "# logger.info(f\"Prompt and Response Length Metrics for Train Split (from file):\")\n",
    "# logger.info(f\"Prompt - Min: {metrics_from_file_train['prompt']['min']}, Max: {metrics_from_file_train['prompt']['max']}, \"\n",
    "#             f\"Mean: {metrics_from_file_train['prompt']['mean']:.2f}, Median: {metrics_from_file_train['prompt']['median']}\")\n",
    "# logger.info(f\"Response - Min: {metrics_from_file_train['response']['min']}, Max: {metrics_from_file_train['response']['max']}, \"\n",
    "#             f\"Mean: {metrics_from_file_train['response']['mean']:.2f}, Median: {metrics_from_file_train['response']['median']}\")\n",
    "\n",
    "# logger.info(f\"Prompt and Response Length Metrics for Test Split (from file):\")\n",
    "# logger.info(f\"Prompt - Min: {metrics_from_file_test['prompt']['min']}, Max: {metrics_from_file_test['prompt']['max']}, \"\n",
    "#             f\"Mean: {metrics_from_file_test['prompt']['mean']:.2f}, Median: {metrics_from_file_test['prompt']['median']}\")\n",
    "# logger.info(f\"Response - Min: {metrics_from_file_test['response']['min']}, Max: {metrics_from_file_test['response']['max']}, \"\n",
    "#             f\"Mean: {metrics_from_file_test['response']['mean']:.2f}, Median: {metrics_from_file_test['response']['median']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions in test data: 1078\n",
      "Total mismatches: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Filepath for the test dataset\n",
    "test_data_file = \"/Users/aravadikesh/Documents/GitHub/MedQA_DPO/data/synthetic_medqa_data/sft_data_test.jsonl\"\n",
    "\n",
    "# Load the test dataset\n",
    "def load_test_data(filepath):\n",
    "    test_data = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "            test_data.append(data)\n",
    "    return test_data\n",
    "\n",
    "# Extract the answer label (e.g., A, B, C, D) from the response\n",
    "def extract_answer_label(response):\n",
    "    return response.strip()[0].upper()  # Extract the first character (e.g., A, B, C, D)\n",
    "\n",
    "# Compare answers using substring matching for questions and ignoring explanations\n",
    "def compare_answers(test_data, reference_data):\n",
    "    mismatches = []\n",
    "    for test_entry in test_data:\n",
    "        test_prompt = test_entry[\"prompt\"]\n",
    "        test_response = extract_answer_label(test_entry[\"response\"])\n",
    "\n",
    "        # Find the corresponding question in the reference dataset using substring matching\n",
    "        reference_entry = next((ref for ref in reference_data if ref[\"question\"] in test_prompt), None)\n",
    "        if reference_entry:\n",
    "            reference_response = reference_entry[\"answer_idx\"]  # Directly use the answer_idx from the reference\n",
    "            if test_response != reference_response:\n",
    "                mismatches.append({\n",
    "                    \"question\": reference_entry[\"question\"],\n",
    "                    \"test_prompt\": test_prompt,\n",
    "                    \"test_response\": test_response,\n",
    "                    \"reference_response\": reference_response\n",
    "                })\n",
    "        else:\n",
    "            mismatches.append({\n",
    "                \"test_prompt\": test_prompt,\n",
    "                \"test_response\": test_response,\n",
    "                \"reference_response\": \"Not Found in Reference Data\"\n",
    "            })\n",
    "    return mismatches\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load the test dataset\n",
    "    test_data = load_test_data(test_data_file)\n",
    "\n",
    "    # Load the reference dataset (GBaker test set)\n",
    "    dataset = load_dataset('GBaker/MedQA-USMLE-4-options', split='test')\n",
    "\n",
    "    # Prepare reference data\n",
    "    reference_data = []\n",
    "    for entry in dataset:\n",
    "        question = entry[\"question\"]\n",
    "        answer_idx = entry[\"answer_idx\"]  # This is already the correct answer label (e.g., 'A', 'B', etc.)\n",
    "        reference_data.append({\n",
    "            \"question\": question,\n",
    "            \"answer_idx\": answer_idx\n",
    "        })\n",
    "\n",
    "    # Compare answers\n",
    "    mismatches = compare_answers(test_data, reference_data)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Total questions in test data: {len(test_data)}\")\n",
    "    print(f\"Total mismatches: {len(mismatches)}\")\n",
    "    if mismatches:\n",
    "        print(\"Mismatches:\")\n",
    "        for mismatch in mismatches[:10]:  # Show up to 10 mismatches\n",
    "            print(f\"Reference Question: {mismatch['question']}\")\n",
    "            print(f\"Test Prompt: {mismatch['test_prompt']}\")\n",
    "            print(f\"Test Response: {mismatch['test_response']}\")\n",
    "            print(f\"Reference Response: {mismatch['reference_response']}\")\n",
    "            print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Found and saved 59 skipped entries to gemma3_data/skipped_entries.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Paths\n",
    "input_file = \"gemma3_data/gemma3_dpo_train_data.jsonl\"\n",
    "scored_file = \"gemma3_data/gemma3_dpo_scored_data.jsonl\"\n",
    "skipped_output_file = \"gemma3_data/skipped_entries.jsonl\"\n",
    "\n",
    "# Load original input\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    original_lines = f.readlines()\n",
    "\n",
    "# Load scored output\n",
    "with open(scored_file, 'r', encoding='utf-8') as f:\n",
    "    scored_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Build a set of prompts that were successfully scored\n",
    "scored_prompts = set(entry[\"prompt\"] for entry in scored_data)\n",
    "\n",
    "# Now find skipped entries\n",
    "skipped_lines = []\n",
    "for line in original_lines:\n",
    "    try:\n",
    "        entry = json.loads(line)\n",
    "        if entry.get(\"prompt\") not in scored_prompts:\n",
    "            skipped_lines.append(line)\n",
    "    except json.JSONDecodeError:\n",
    "        skipped_lines.append(line)  # Also treat invalid JSON as skipped\n",
    "\n",
    "# Save the skipped entries\n",
    "with open(skipped_output_file, 'w', encoding='utf-8') as f:\n",
    "    for line in skipped_lines:\n",
    "        f.write(line)\n",
    "\n",
    "print(f\"Done! Found and saved {len(skipped_lines)} skipped entries to {skipped_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for dpo dataset:\n",
      "total_samples: 881\n",
      "avg_prompt_length: 865.42\n",
      "avg_response_length: 245.78\n",
      "min_prompt_length: 263\n",
      "max_prompt_length: 1406\n",
      "min_response_length: 181\n",
      "max_response_length: 406\n",
      "\n",
      "Metrics for test dataset:\n",
      "total_samples: 1078\n",
      "avg_prompt_length: 879.71\n",
      "avg_response_length: 245.88\n",
      "min_prompt_length: 310\n",
      "max_prompt_length: 1474\n",
      "min_response_length: 152\n",
      "max_response_length: 441\n",
      "\n",
      "Metrics for train dataset:\n",
      "total_samples: 7036\n",
      "avg_prompt_length: 872.69\n",
      "avg_response_length: 245.29\n",
      "min_prompt_length: 270\n",
      "max_prompt_length: 1843\n",
      "min_response_length: 148\n",
      "max_response_length: 558\n",
      "\n",
      "Metrics for val dataset:\n",
      "total_samples: 879\n",
      "avg_prompt_length: 866.94\n",
      "avg_response_length: 245.63\n",
      "min_prompt_length: 361\n",
      "max_prompt_length: 1682\n",
      "min_response_length: 169\n",
      "max_response_length: 433\n"
     ]
    }
   ],
   "source": [
    "dpo = '/Users/aravadikesh/Documents/GitHub/MedQA_DPO/data/original_datasets/dpo_dataset.json'\n",
    "test = '/Users/aravadikesh/Documents/GitHub/MedQA_DPO/data/original_datasets/test_dataset.json'\n",
    "train = '/Users/aravadikesh/Documents/GitHub/MedQA_DPO/data/original_datasets/train_dataset.json'\n",
    "val = '/Users/aravadikesh/Documents/GitHub/MedQA_DPO/data/original_datasets/val_dataset.json'\n",
    "\n",
    "# Load all datasets\n",
    "datasets = {\n",
    "    'dpo': dpo,\n",
    "    'test': test,\n",
    "    'train': train, \n",
    "    'val': val\n",
    "}\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "# Calculate metrics for each dataset\n",
    "for name, filepath in datasets.items():\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "            \n",
    "        # Basic count metrics\n",
    "        metrics[name] = {\n",
    "            'total_samples': len(data),\n",
    "            'avg_prompt_length': np.mean([len(str(item.get('prompt', ''))) for item in data]),\n",
    "            'avg_response_length': np.mean([len(str(item.get('response', ''))) for item in data]),\n",
    "            'min_prompt_length': min([len(str(item.get('prompt', ''))) for item in data]),\n",
    "            'max_prompt_length': max([len(str(item.get('prompt', ''))) for item in data]),\n",
    "            'min_response_length': min([len(str(item.get('response', ''))) for item in data]), \n",
    "            'max_response_length': max([len(str(item.get('response', ''))) for item in data])\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {name} dataset: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Print metrics\n",
    "for dataset_name, dataset_metrics in metrics.items():\n",
    "    print(f\"\\nMetrics for {dataset_name} dataset:\")\n",
    "    for metric_name, value in dataset_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{metric_name}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"{metric_name}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
